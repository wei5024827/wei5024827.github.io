### 自我介绍

我叫，山西吕梁人，今年28岁，毕业于中北大学计算机科学与技术专业，目前就职紫川软件，驻场在平安普惠大数据项目组。这边项目叫BDSP，分离线和实时部分，离线部分主要用到了hive和spark这两种技术。Hive做数据存储，SQL的解析，Spark做数据计算与数据的同步

实时部分主要的技术架构就是Flink做流数据计算，clickhouse和Hbase做数据存储

除此以外，我对Kylin，Druid这些有所了解，不同场景下用过FlinkCDC，flume，Canal，Maxwell等等数据同步工具。

#### 为什么要离职？

- 平安普惠职场搬到了唐镇
- 平安普惠的所有技术都已经学到了，想看看别的公司是怎么做的

### 项目简述

![image-20220627123433204](C:\Users\wei50\AppData\Roaming\Typora\typora-user-images\image-20220627123433204.png)





![image-20220627123818958](C:\Users\wei50\AppData\Roaming\Typora\typora-user-images\image-20220627123818958.png)

#### 碰到过什么问题？

##### 离线问题

数据倾斜：

- 背景需求：申请基础模型在加字段的时候需要关联客户预约表和客户申请表，关联字段是预约渠道，申请渠道。
- 问题及原因：在发版日跑的时候，发现任务跑了两个班小时还没跑完，看azkaban执行日志，在某个stage执行时出现了停在99%不动的情况，是数据倾斜问题。
- 解决过程：
  - 与业务沟通后拿到oracle业务库，按照预约渠道的预约数量降序排序后，排名第一的数量三千多万挑，第二的200多万条，第三之后的均是十万级。且客户预约表和申请表都是亿级的大表
  - 两张表都进行过滤，预约量前2的存入一组，然后join，之后的放入一组进行join，然后将两组进行union all

数据量过大：

- CMP每日案件明细数据量太大，导致全量跑批超时，影响下游表

- 原因：HIVE表大都是按照全量跑的，业务错误的估计了数据量，导致跑批超时

- 解决：与业务沟通后，每日只需计算三天内的数据，T-4到T-1的数据，Sqoop按日增抽取数据，表按日分区，编写脚本，每日删除T-5的分区数据。

##### 实时问题

- 现象：布尔类型过滤的时候报错
  - 当时写的代码时 deteled = ‘false’，flink sql中要写为 deted is false
- 现象：flink sql连接kafka时提示链接超时
  - 在配置中添加链接超时时间90000ms
- 现象：flink从kafka拿到数据后执行报错窗口类型报错
  - 窗口的时间戳类型必须为timestamp，而我写的时bigint
- 现象：flink sql在解析json时报错
  - 后来引入flink-json的pom包后解决了，但是官网说json内置，不清楚啥情况
- 现象：从hive中读取客户模型表，因为表过大，导致某些数据在flink在窗口时间无法聚合。
  - 解决：首先实时的Hbase集群还在审批中，数仓集群中导出数据集市的mysql实时数仓没有权限读取数据
  - 把客户模型表涉及到的表从上游读取到mysql cdc中，然后flink去做关联处理拿到一个和离线数仓一模一样的表，最后写入到状态中，当作维表。
- Ck建表失败，显示ck表已经存在，但是排查目录后发现并不存在
  - 所以怀疑有可能时zk元数据问题
  - 让运维把zk中的元数据删掉

#### 具体讲一个做的案例

三号五星项目，是普惠的销售BU为了给销售员做的一个实时看板，主要记录每个销售组以及每个销售员实时的销售情况

这个项目需要流批两套系统协作。

首先在离线系统中，会使用到ncms电销金服表，三好五星小组表，城市基表数等表建立销售员模型表。这张表来记录项目中销售BU的每位销售的基础信息。

陆慧融车抵客户表和房抵客户表，鲁班来电用户表等客户来源渠道表建立客户模型表。

这是离线系统中聚合计算的表，供实时数据聚合使用

在流处理系统中，会从oracle实时同步电销记录表，预约模型表，放贷模型表等等，当这些表会在dw层与离线系统中的销售员工表和客户表做聚合。

这个项目设计到的上游业务表有20张左右，最终聚合的指标5分钟内，省，市，县，门店放贷额度，放贷成功率，销售放贷额度，成功率，贷款产品放款排行，贷款产品放款成功率排行等等指标。



#### 建模方法论

- **分层**
  - **为什么要分层？**
    - **清晰数据结构**：每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。
    - **数据血缘追踪**：简单来讲可以这样理解，我们最终给业务诚信的是一能直接使用的张业务表，但是它的来源有很多，如果有一张来源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围。
    - **减少重复开发**：规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算。
    - **把复杂问题简单化**：一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。
    - **屏蔽原始数据的异常对业务的影响**：不必改一次业务就需要重新接入数据
- **建模**
  - **什么是建模？**
    - 本质：决定了数据存储的方式，表的设计
  
  - **为什么要建模？**
  
    - **大数据系统需要数据模型方法来帮助更好地组织和存储数据，以便在性能、成本、效率和质量之间取得最佳平衡。**
    - **性能**：良好的数据模型能帮助我们快速查询所需要的数据，减少数据的I/O吞吐
    - **成本**：良好的数据模型能极大地减少不必要的数据冗余，也能实现计算结果复用，极大地降低大数据系统中的存储和计算成本
    - **效率**：良好的数据模型能极大地改善用户使用数据的体验，提高使用数据的效率
    - **质量**：良好的数据模型能改善数据统计口径的不一致性，减少数据计算错误的可能性
  
  - **有哪些建模方法？**
    - **ER模型**：从全企业的高度设计一个 3NF 【三范式】模型，用实体关系模型描述企业业务，满足业务需求的存储
    - ==**维度模型**==：从分析决策的需求出发构建模型，为分析需求服务，重点关注用户如何更快速的完成需求分析，具有较好的大规模复杂查询的响应性能
    - **Data Vault**：ER 模型的衍生，基于主题概念将企业数据进行结构化组织，并引入了更进一步的范式处理来优化模型，以应对源系统变更的扩展性
    - **Anchor**：一个高度可扩展的模型，核心思想是所有的扩展知识添加而不是修改，因此将模型规范到 6NF，基本变成了 k-v 结构化模型
  
  - **怎么构建维度模型步骤**？
    - **选择业务过程**：你要做什么？
      - 我要做订单分析、用户分析、商品分析
    - **声明粒度**：你的分析基于什么样的颗粒度？
      - 实现每天的订单分析、用户分析、商品分析
    - **确认环境的维度**：你的整体有哪些维度？
      - 时间、地区、产品、栏目、平台、用户……
    - **确认用于度量的事实**：你要基于这些维度构建哪些指标？
      - 订单个数、订单金额、新增用户个数、总用户个数、活跃用户个数、流失用户个数、热门商品Top10
  
  - ==**具体维度建模的实施流程是什么？**==
  
    - **1-需求调研**：业务调研和数据调研
  
      - 了解整个业务实现的过程：用户访问、注册、登陆、搜索、浏览、添加购物车、提交订单、支付
      - 收集所有数据使用人员对于数据的需求：数据分析师的报表需求等
      - 整理所有数据来源
  
    - **2-划分主题域**：面向业务将业务划分主题
  
      - 用户与店铺域、商品域、交易域、客服域、信用风控域、采购分销域
  
    - **3-构建维度总线矩阵**：明确每个业务主题对应的维度关系
  
    - **4-明确指标统计**：明确所有原生指标与衍生指标
  
      - **原生指标**：原子指标和度量含义相同，基于某一业务事件行为下的度量，是业务定义中不可再拆分的指标，具有明确业务含义的名称，如支付总金额
  
      - **衍生指标**：基于原子指标添加了维度：近7天的支付总金额等
  
    - **5-定义事实与维度规范**
  
      - 命名规范、类型规范、设计规范等
  
    - **6-代码开发**
  
      - 实现具体的代码开发
      - 指标：不同的行业指标不一样，计算方式也不一样
      - 维度：分组字段

### 数据采集

#### sqoop遇到的问题

- 数据全部为null，建表和导入字段的分隔符不一致造成的

- 导入hive数据量变多
  - map并行度设置了5，设置为1就数据量正确了，因为没有指定--split-by字段，分区时sqoop自动按照主键进行数据分配，导致了数据重复，指定--split-by为int型的字段
- 内存溢出的问题，报错显示物理内存不够，虚拟内存不过，
  - 如果提示物理内存不足，在yarn.site设置内存大于报错中所需要的内存数量即可
  - 如果虚拟内存不足，直接关闭yarn对虚拟内存的设置

sqoop采集怎么保证数据一致性？

- 写脚本，采集完成后统计hive的数据量和上游业务库的数据量

#### 实时采集碰到过什么问题？

- flinkcdc 1.0时实时监控Mysql binlog的内容时，会对Mysql加锁，导致其他客户端做不了dml操作1.0
  - 解决：用了另一个mysql库

#### 拉链表的实现

- 功能：解决事实中渐变维度发生变化的问题，通过时间来标记维度的每一种状态，存储所有状态

- 实现
  - step1：先采集所有增量数据到更新表中
  - step2：将更新表的数据与老的拉链表的数据进行合并写入一张临时表
  - step3：将临时表的结果覆盖到拉链表中

### 优化

#### hive的优化

Hive调优一般是解决数据处理效率问题，可以通过一些配置来加快运行速度

##### 资源优化

- fetch抓取机制，这种机制开启之后，sql能不走mr就不走mr，比如开启后全局查找，字段查找，limit查找，sql都不会走mr
- 开启推测执行机制，一个job底层有多个task执行，某些task执行慢，推测机制可以利用算法找出这些task，然后设置备份，两份一起执行，谁先执行完，谁作为最终的处理结果

##### sql，join优化

- hive 如果在执行join时会有两阶段的join，map端join和reduce端的join，可以设置开启map join，然后给表设置阈值，指定不超过多少阈值可以走mapjoin
- 如果时大表join大表

  - 可以先对空key进行过滤
- 如果某些key值过多，可以用加盐的方式  case when key = xxx then

##### 小文件优化

- 场景
  - 动态分区插入数据会导致map数量剧增
  - reduce阶段数量过多，也会导致小文件剧增
  - 数据源本身包含了大量的小文件
- 原因：
  - 小文件会开很多map，一个map对应一个jvm，会浪费大量资源
  - 在HDFS，小文件对象占用150字节，如果小文件过多，会造成namenode的内存溢出
- 优化：
  - 使用archive命令把小文件归档
  - Hive配置参数有个叫CombineInputFormat，作用就是在执行map前先合并小文件，可以设置打开
  - 设置map和reduce结束后合并小文件
  - Hive sql 可以用alter table 表名 concatenate 来对表的小文件进行合并。

##### 数据倾斜优化

- 一般在group by key后会出现数据倾斜

  - 开启map端预聚合这个参数
  - 然后设置map端预聚合的条目数据
  - 如果有数据倾斜的时候进行负载均衡
    - 原理：碰到数据倾斜再开启一个MR程序，将倾斜的数据发送到各个reduce中，打散后进行局部聚合
    - 完成后再启动一个mapreduce程序，将上一步的聚合结果汇总起来再进行一次聚合
- 过滤：可以先把少数key全部过滤掉，然后只保留数据量多的key进行聚合，让并行的task所有资源都去处理这些key的数据，处理完成后再处理少数的key，最后将两部分数据进行union

#### flink的优化

##### 资源配置优化

- 内存设置，jobmanager内存，taskmanager内存
- 并行度设置，
  - source端并行度尽量和kafka topic分区数相等
  - transform端并行度如果作业过多可以往高一点配
  - sink端可以按照下游kafka topic分区数或者其他组件的分区数配置

##### 反压处理

- 利用web ui或者grafana等监控组件查看是否反压
- 一般反压都是负载不均衡导致了数据倾斜
  - keyby之前，查看kafka topic中的分区数据量，如果是这个原因，需要用到shuffle，rebalance算子重新分配数据
  - 如果是keyby之后的窗口出现数据倾斜，可以用两阶段聚合的方式，第一阶段给key加盐进行聚合，完成后第二阶段去掉加盐，然后再聚合一次

##### flink sql调优

- 开启minibatch，微批处理，减少对state的访问，提高吞吐量
- 开启localgloabl，可以将aggregate等聚合算子分为local+gloabl两阶段去处理，解决热点问题
- 开启split distinct，localgolable对本地去重率并不高，split distinct可以将本地数据重新打散去重再聚合
- case when换位filter，filter可以共享一个对象实例，减少对状态的访问

#### spark 优化

##### 配置优化

- 对多次使用的RDD进行持久化操作cache，persist，checkpoint，这样第二次使用RDD数据的时候就不用再进行重新计算
- 使用parallelize等方法设置提高并行度，增加CPU处理的核数，来提高程序的运行效率
- 广播共享数据，对于数据量不大的文件，要执行join操作时，可以广播到各个节点，减少网络资源消耗
- 数据本地化，调整spark等待task可以进行本地化的时间，来提升数据本地化的级别
- 对shuffle的一些配置进行设置，达到调优的效果
  - 调整map task缓冲区的大小，shuffle过程中会有一个数据进入缓冲区，溢写磁盘的过程。适当的把缓冲区调大，可以减少溢出小文件的数量
  - 调整reduceTask的拉取缓存，调大点可以加快reduce阶段的执行速度
  - 拉取失败最大重试次数，有些时候网络不稳定会导致数据拉取失败，可以多次重试
  - 调整shuffle的内存占用，如果程序较多使用了shuffle操作，可以适当调大
  - shufflemanager，默认是sort，如果不需要业务逻辑中不需要排序，可以改为hash
  - 使用hash shuffle，会产生大量的小文件，可以开启consolidateFiles，可以将小文件大幅度合并。

##### spark sql，数据倾斜优化

在shuffle中，遇到join或者按key聚合的操作，把大量相同的key拉取到一个task处理，这时就发生了数据倾斜

- 过滤少数几个导致数据倾斜的key，使用场景比较少，大多数情况下导致倾斜的key还是很多的
- 提高shuffle read的并行度，增加shuffle read task的数量，可以重新把不同的key分配到不同的分区。可以有效环节的数据倾斜
- 如果碰到聚合类算子，可以用两阶段聚合，局部聚合再全局聚合的方式解决数据倾斜
- 将reduce join转为map join，可以将数据量较小的RDD通过广播的方式发送到每个节点，然后再join，这样就避免了shuffle过程，完全避免了数据倾斜的发生。

### Hive

#### Hive开发中遇到的函数

- 聚合函数，sum，avg，count，min，max
- 分析函数：lead，lag，ntile，rank，row_number , dense_rank , first_value , last_value

![image-20220609094249005](C:\Users\wei50\AppData\Roaming\Typora\typora-user-images\image-20220609094249005.png)

- 字符串操作
  - 截取：substring(str, pos[, len])
  - 分割：split(str, regex)
- json函数
  - get_json_object(json_txt, path)
- 正则表达式替换函数：regexp_replace(str, regexp, rep)
- 日期函数：unix_timestamp，from_unixtime，date_add，date_sub，datediff
- 条件函数：isnull，isnotnull，nvl(T value, T default_value)，COALESCE(T v1, T v2, ...)，case a when b then c end
- 类型转换函数：cast



#### hive常见的压缩格式和数据格式

- 压缩格式：snappy，bzip2，gzip，lzo

- 数据格式：Textfile，SequenceFile，ORC，Parquet

- orc与parquet的区别？
  - orc支持的查询引擎只有pig，hive，Parquet可以支持flink，spark，impala等等
  - Orc支持ACID和update等实务操作，Parquet是不支持的

#### MR执行流程 

- map阶段

  - mapper从hdfs读数据，将数据切块，默认切块大小为hdfs的块大小，利用map函数将块中的数据解析成kv。

- shuffle阶段

  - 对kv按key进行Hash分区，分区后加载到缓冲区，当缓冲区存储到达80%后，锁定这部分数据，在内存中进行快速排序，再溢出到磁盘，形成有序的小文件。
  - 将磁盘中的小文件合并为整体有序的大文件。
  - reduce拉取属于自己分区的数据，将属于自己的每个maptask的数据进行合并并排序

- reduce阶段

  最终按照key值分组排序，将结果写入HDFS文件系统中

#### 行列转换

- 多行转单列
  - collect_set：去重
  - collect_list：不去重

- 多列转单行

  - explode+ lateral view

### spark

#### spark算子

- 窄依赖算子：

  1.map算子

  2.flatMap算子

  3.mapPartitions算子

- 宽

  - union
  - groupbykey
  - distinct
  - cache
  - persist

#### Spark 的任务提交流程你熟悉吗？

- 客户端向Master 发送请求
- Master 收到申请资源的请求后，让 worker 开启对应的 executor 进程（计算资源）。
- executor 进程会向 Driver 端反向注册请求，然后申请要计算的 Task。
- 然后Driver 开始SparkContext 上下文对象，同时创建了分别是 DAGScheduler 和 TaskScheduler。
- DAGScheduler 根据rdd之间的依赖关系，构建DAG图， 按照宽依赖，进行 stage 划分，每一个 stage 内部有很多可以并行运行的 task 线程，然后把这些并行运行的 task 线程封装在一个 taskSet 集合中，最后将taskset这个集合广播到taskscheduler
- TaskScheduler 对象获取得到这些 taskSet 集合之后，按照 多个 stage 之间的依赖关系和顺序，把每一个 Task 依次提交到 worker 节点上的 Executor 进程中运行。
- 任务完成后，回收资源

#### Spark shuffle

spark程序在执行过程中遇到宽依赖算子会执行shuffle

shuffle的本质是将不同分区的RDD按照Key值进行重新分区。

shuffle有两个阶段，第一个阶段shuffle write，第二个阶段shuffle read

在执行shuffle write时，首先程序会判断会不会执行bypass运行机制，如果程序的map task的个数是否小于BypassMerge的值，如果小于的话，就会执行bypass机制，也就是执行hash shuffle

如果大于的话，默认执行sort shuffle

以sort shuffle为例，不同分区的RDD会先在内存中进行一次排序，然后进入缓冲区，当在缓冲区达到阈值时，会溢写到磁盘，形成一个小文件，当这个分区的数据都溢写完成后，所有的小文件合并成一个大文件，同时生成一个索引文件，供shuffle read快速查询Key对应的数据使用。

进入shuffle read阶段后，根据不同的算子，执行不同的功能。比如groupbykey：做分组  sortbykey：做排序。repatition：做分区。

这就是一个完整的shuffle过程。

### Kafka

#### kafka如何生产数据？

- 首先看生产者有没有指定分区，指定了分区，按照分区生产数据
- 没有指定分区，看数据的key是否为Null，如果是Null，默认情况下回使用黏性分区（最新版本）或者轮询分区
- 如果Key的值不为Null，按照MUR取余的算法指定数据的分区
- 当然也可以自定义分区器，重写partition方法

#### Kafka保证生产的一致性语义？

生产者发送数据时等待kafka返回ack，来确保数据不丢失

ack有三种响应模式

0：不需要ack回应

1：发送一条数据，这条数据只要在kafka中有了一条副本，就返回ack

all：发送一条数据，这条数据在kafka中所有副本都同步好了，才返回ack

ack并不能保证数据不重复，比如kafka在返回ack的过程中ack因为网络故障丢掉了，生产者就认为kafka没有收到数据，会再把这份数据重新传一份给kafka，造成了数据重复。

这时需要使用幂等性机制，在生产者给kafka生产数据时每条数据都加上编号，kafka每次接收数据时会检查这条数据的id是否重复

#### 如何消费数据？消费数据如何保证一致性？

- 按照topic消费，在代码中指定subscribe Topic名称即可
- 按照分区消费，在代码中写明topic的分区数即可
- 按照offset消费，
  - 第一次消费的时候，根据属性来决定消费的位置
    - latest：默认，从Topic每个分区的最新位置开始消费
    - earliest：从offset为0的位置开始消费
    - none
  - 之后消费，按照上一次消费offset的位置继续消费，记录offset消费位置的数据会被同一统计进一个特殊topic：__consumer_offsets
  - 工作中：自己用消费者来管理offset，自己在代码中将offset存储在外部系统：MySQL、ZK、Redis

### Flink

#### flink常用算子

API: Map，FlatMap，Filter，KeyBy，Window，Split，shuffle，rebalence,global,forward

SQL:

![image-20220628211052150](C:\Users\wei50\AppData\Roaming\Typora\typora-user-images\image-20220628211052150.png)

#### flink怎么实现端对端的一次性语义

依靠幂等性和两阶段提交

#### Flink的状态CheckPoint的执行流程？Flink的容错机制？

- jobmanager创建checkpoint协调器
- 协调器向各个任务发送barrier
- 收到barrier后制作state快照并进行存储，存储完成后向协调器汇报，把Barrier发送给下游
- 重复上面的步骤，直到flink的job都完成
- 协调器收到所有任务执行OK的汇报结果，完成checkpoint

### Flink遇到大状态如何优化？

- flink web ui两个指标： **subtask checkpoint 延迟时间**和**barrier 对齐的缓存数据量**来看是否又大状态
- 调整Checkpoint 参数，如果 Checkpoint 持续的时间经常大于 Checkpoint 的时间间隔，系统会不断进行 Checkpoint（一旦完成 Checkpoint，则立即启动新的 Checkpoint） 。这可能意味着 Flink 的资源经常被用于 Checkpoint，影响算子 Operator 正常处理业务数据。该情况对使用异步 Checkpoint 的作业较小，但影响作业的整体性能。为了避免该情况，作业可以自定义 Checkpoint 的最小暂停时间。
- 增量RocksDB，如果状态后端是RocksDB，可以选择增量Checkpoint，**增量 Checkpoint 是利用 RocksDB 的内部压缩机制。该机制可以随着时间的推移>进行自动整合，导致**增量 Checkpoint 的历史记录不会无限期增长，而是最终自动合并和清除旧的 Checkpoint**。
- 压缩状态数据，用snappy压缩
- 作业恢复状态
  - 本地checkpoint目录保留，本地副本与RocksDB共享目录
  - 对于活跃的文件，使用增量 Checkpoint 进行本地恢复

### Kylin架构

![image-20220627123652832](C:\Users\wei50\AppData\Roaming\Typora\typora-user-images\image-20220627123652832.png)



### Clickhouse

#### clickhouse为什么快？

- 列式存储，聚合查询很快
- 有很多引擎，支持各种场景，
  - TinyLog：存储在磁盘上，不支持索引，没有并发，一般小表
  - Memory：内存引擎，不支持索引，性能高
  - 最常用的MergeTree
    - 允许Ck是分布式的
    - 主键用的是稀疏索引
      - 占用空间小
      - 而且有序


- 表引擎MergeTree
  - MergeTree：最常用的一种ck表引擎
    - partition by：分区字段，MergeTree允许CK表是分布式的
    - primary key：必须是order by后面的字段：因为MergeTree性能高的原因是读取很快，有稀疏索引，而稀疏索引必须要对主键排序
      - 稀疏索引
        - 占用空间小，查询快
        - 必须是有序的
    - Order by ：指定排序字段
    - TTL：过期时间


- ReplacingMergeTree
  - 去重的MergeTree
  - 根据order by的字段进行去重
  - 使用方式：engine=ReplacingMergeTree(字段)
    - 字段：表示该条数据的版本，如果重复，保留这个字段版本最新的
- SummingMergeTree
  - 聚合的MergeTree
  - 使用方式：engine=SummingMergeTree(字段)
    - 字段：表示要聚合的字段
- CK SQL

  - with rollup：从右到 左去除维度进行小计
  - with cube：从右到 左去除维度进行小计，再从左到右去除维度进行小计
  - with totals：只计算合计

### SQL

![image-20220609094249005](C:\Users\wei50\AppData\Roaming\Typora\typora-user-images\image-20220609094249005.png)
