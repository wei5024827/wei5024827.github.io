#### 自我介绍：

我叫XX，来自山西吕梁，今年28岁，毕业于中北大学计算机科学与技术专业，目前就职紫川软件，驻场在平安普惠大数据项目组。我比较擅长hadoop体系的大数据框架，比如目前比较火的，Hive，Spark批处理框架，Flink流处理框架，还有这些框架所衍生出来的工具，SQOOP，Flink CDC，Kylin等等，还有HBase，Clickhouse，Druid等等OLAP的数据库都有接触过。

在工作中开发主要以sql为主。这次找工作原因是

#### 项目简述

BDSP2.0是平安普惠的一个大数据平台，主要是通过这个平台来为普惠各大APP，销售，贷后，风控，陆慧融等部门提供数据分析及决策支持，这个平台分为批处理架构和流处理架构两部分，两个部分协同，为各BU提供数据支持

其中批处理架构（离线数仓）分为两层，DW层和DM层，主要的流程是这样的，在DW层从上游业务数据库利用spark-submit全量同步数据到Hive，作为DWD层（原始数据明细层），然后根据业务需求对DWD的数据进行处理作为DWM层，DWM层与DIM层的数据进行聚合后得到dmm层，dmm用sqoop导数到mysql，然后给下游各个BU使用。最后各bu会把最终的结果呈现在SAS，帆软，tableau，还有普惠内部BU开发的应用系统中，也会将dim的某些维表创建一份parquet格式的，供流处理使用。

流处理架构分为三层ods，dw，dm层，首先拿到需求后会同步上游表，在mysql cdc中建立一张与上游一摸一样的表，然后动态监控这张表的变化，增量同步在mysql -cdc中，mysql-cdc再把增量数据同步到kafka，到kafka这一层作为ods层，然后利用flink sql对接kafka，对kafka的数据进行加工转换，如果遇到维表需要聚合，就从Hive中拿维表，然后将结果写入clickhouose中，这一层作为dws层，给各BU提供即席查询服务

这是项目的一个简单的框架介绍

我所负责的内容大致有两块，批处理和流处理的dw层的开发

批处理DW层，版本需求澄清后，分析需求，理清DW中指标的逻辑关系，制作DW层表模型，利用sqoop同步dwd表，利用hive sql开发dwm层表，并开发调度脚本。

流处理：同步上游表到mysql cdc，开发flink sql完成对指标的计算，并把最终数据写入ck表，根据数据量以及QPS配置Flink任务执行所需要的资源，Taskmanager memory，Taskmanager slot个数等等。上线后生产环境查看是否背压，然后做出调整。



#### 具体讲一个做的案例

三号五星项目，是普惠的销售BU为了给销售员做的一个实时看板，主要记录每个销售组以及每个销售员实时的销售情况

这个项目需要流批两套系统协作。

首先在离线系统中，会使用到ncms电销金服表，三好五星小组表，城市基表数等表建立销售员模型表。这张表来记录项目中销售BU的每位销售的基础信息。

陆慧融车抵客户表和房抵客户表，鲁班来电用户表等客户来源渠道表建立客户模型表。

这是离线系统中聚合计算的表，供实时数据聚合使用

在流处理系统中，会从oracle实时同步电销记录表，预约模型表，放贷模型表等等，当这些表会在dw层与离线系统中的销售员工表和客户表做聚合。

这个项目设计到的上游业务表有20张左右，最终聚合的指标5分钟内，省，市，县，门店放贷额度，放贷成功率，销售放贷额度，成功率，贷款产品放款排行，贷款产品放款成功率排行等等指标。



#### 项目中状态存储在什么地方？checkpoint在哪儿？

然后从将状态存储在rocksDB，checkpoint存储在hdfs文件系统中



#### 碰到过什么问题？

##### 离线

数据倾斜：

- 背景需求：申请基础模型在加字段的时候需要关联客户预约表和客户申请表，关联字段是预约渠道，申请渠道。
- 问题及原因：在发版日跑的时候，发现任务跑了两个班小时还没跑完，看azkaban执行日志，在某个stage执行时出现了停在99%不动的情况，是数据倾斜问题。
- 解决过程：
  - 与业务沟通后拿到oracle业务库，按照预约渠道的预约数量降序排序后，排名第一的数量三千多万挑，第二的200多万条，第三之后的均是十万级。且客户预约表和申请表都是亿级的大表
  - 两张表都进行过滤，预约量前2的存入一组，然后join，之后的放入一组进行join，然后将两组进行union all

数据量过大：

- CMP每日案件明细数据量太大，导致全量跑批超时，影响下游表

- 原因：HIVE表大都是按照全量跑的，业务错误的估计了数据量，导致跑批超时

- 解决：与业务沟通后，每日只需计算三天内的数据，T-4到T-1的数据，Sqoop按日增抽取数据，表按日分区，编写脚本，每日删除T-5的分区数据。

##### 实时

- 现象：布尔类型过滤的时候报错
  - 当时写的代码时 deteled = ‘false’，flink sql中要写为 deted is false

- 现象：flink sql连接kafka时提示链接超时
  - 在配置中添加链接超时时间90000ms

- 现象：flink从kafka拿到数据后执行报错窗口类型报错
  - 窗口的时间戳类型必须为timestamp，而我写的时bigint
- 现象：flink sql在解析json时报错
  - 后来引入flink-json的pom包后解决了，但是官网说json内置，不清楚啥情况
- 现象：从hive中读取客户模型表，因为表过大，导致某些数据在flink在窗口时间无法聚合。
  - 解决：首先实时的Hbase集群还在审批中，数仓集群中导出数据集市的mysql实时数仓没有权限读取数据
  - 把客户模型表涉及到的表从上游读取到mysql cdc中，然后flink去做关联处理拿到一个和离线数仓一模一样的表，最后写入到状态中，当作维表。
- Ck建表失败，显示ck表已经存在，但是排查目录后发现并不存在
  - 所以怀疑有可能时zk元数据问题
  - 让运维把zk中的元数据删掉

  

  

  

  

  



