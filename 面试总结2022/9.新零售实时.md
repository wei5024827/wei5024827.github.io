### 洋码头实时数据分析项目简述

洋码头是一个做海外购的电商平台，实时数据分析项目只要是为了应对每日直播，节假日促销时带来的订单量剧增，以及对用户行为做出实时分析。

数据来源主要分为行为日志和业务数据两部分。

行为日志数据通过spring boot集群拦截日志服务器的行为数据到kafka，行为数据主要有用户的id，来源页面，访问页面，机型，客户端等等信息

业务数据来源mysql，通过配置binlog，让flinkcdc实时监控数据库的变化，并将变化数据采集到Kafka

以上行为数据和业务数据采集完成后共同构成了ods层



接下来是DWD层，flink消费kafka中ODS层的数据

针对于行为日志topic中过来的数据，做了三件事

- 脏数据清洗，
- 新老用户校验，因为上游日志系统中新用户is_new字段为1,传入数仓后也为1，但是新用户继续进行操作时
- 数据的分流，将页面信息作为主流，启动日志和曝光日志作为侧边流输出，启动日志数据和曝光日志数据用相关关键词字段来判定，最后将三个流写入到kafka各自的topic中。

针对与业务数据topic中过来的数据，

- mysql业务会新增表，所以做了一个动态配置方案，在mysql中新增一张数据库变化表，flinkcdc动态感知将新表做广播流处理。
- 主流和广播流连接然后进行分流
- 维度表保存在HBase中，作为DIM层
- 一张事实表对应kafka一个topic，作为DWD层

以上两部完成后共同构成了DWD层



接下来是DWM层，DWM层主要计算了访客UV表，跳出明细表，订单宽表和支付宽表

其中访客UV和跳出明细用的是DWD中日志流



访客UV计算方式

- 保留mid每天第一次登陆的数据

  - 设置状态ttl为24h
  - 对last_page_id做判断，last_page_id是用户浏览的上个页面，如果为null，则证明该用户已经跳出，保留这条数据，其他数据都过滤，并将这条数据状态保存下来。
  - 因为状态保存的24h可能是跨天的，所以需要将这条数据与当前日期进行对比，如果一样，保留，不一样就过滤掉
  - 最后将数据写入到kafka中

跳出明细

  -  跳出明细会计算两个数据，跳出次数，和访问次数
  -  用Flink CEP实现
     - 访问次数：将DWD层中所有曝光信息的数据流按照用户id进行keyby，即可计算出每个用户的访问次数
     - 跳出次数用两个条件来判断
       - 上次访问页面为null的，就是跳出数据
       - 在每个用户数据进来的时候设置一个10s的会话窗口，在10s内没有数据的，也是跳出数据
  -  最后将数据写入kafka



订单宽表，将订单按照不同维度进行聚合

- 用到了订单表和订单明细表，两张表需要进行双流join，订单表interval join订单明细表，窗口大小-5，5s
- 双流join完成后关联维度信息，地区维度，用户维度，sku维度，spu维度等等
- 关联完成后写入kafka



支付宽表，

- 双流join，将订单宽表和支付表进行join，得到支付宽表
  - 用支付表去join订单宽表，窗口设置-15min,5min
  - 完成后将支付宽表输出到kafka

以上四个流处理完成后作为DWM层



接下来就是DWS层，按照主题域划分做的数据市场



访客主题宽表

要求的指标有，PV，UV，跳出率，进入页面数，连续访问时常

PV，UV在dwm层都过滤出来了，直接进行聚合

跳出率用dwm层跳出明细计算，跳出次数/访问次数

连续访问市场可以直接用dwd中的日志数据用户的duration time做聚合

拿到这些指标信息，与phonix表中的维度表进行分组开窗聚合，用了10s的滚动窗口（Superset只能10s刷新一次）

最后将数据写入clickhouse中



商品主题宽表

需要计算的指标有点击量，曝光量，收藏量，加入购物车，下单量，支付金额，退款量，评价

将每个商品的对应指标数据从各个层拿到以后，所有数据合并为一个流，设置一个5s的水位线

按照维度进行分组，SKU，SPU，品牌，品类，开窗10s，进行聚合

最后也将数据写入到clickhouse中



地区主题宽表

需要计算的指标有，省份订单总量，订单金额，用户量，下单率等等

开窗10s，聚合

写入clickhouse中









### 技术选型

Mysql、SpringBoot、Nginx、kafka、Flink、CK、DS、Grafana、mysql、redis、hbase



#### DB实时采集工具

- FlinkCDC
  - DataStream与FlinkSQL对比

    - DataStream
      - 优点：多库多表
      - 缺点：需要自定义反序列化器
    - FlinkSQL：

      - 优点：不需要自定义反序列化器
    - 缺点：单表查询


- Maxwell

  - 使用方法：

    配置Maxwell配置文件（DB服务地址，库名，表名，分区），启动maxwell往kafka生产数据


- Canal
  - 使用方法：	
    - 配置文件：
      - serverMode=kafka
      - Kafka地址
      - 生产者参数
      - Kafka Topic
    - 启动canal


- FlinkCDC Maxwell Canal三者对比

​		新增多条：Canal会把多条数据合并成一条，在序列化时需要自己炸裂，开发复杂

​		更新操作：Maxwell会把更新字段单玲出来，序列化不方便操作

|               | FlinkCDC       | Maxwell    | Canal          |
| ------------- | -------------- | ---------- | -------------- |
| 断点续传      | CheckPoint     | Mysql      | 本地磁盘       |
| SQL->数据影响 | 无             | 无         | 一对一（炸开） |
| 初始化功能    | 有（多库多表） | 有（单表） | 无             |
| 封装格式      | 自定义         | JSON       | JOSN/自定义    |

#### 日志数据SpringBoot采集

- Controller：拦截用户数据，调用service，相应请求
- Service：调用DAO，加工数据
- DAO：获取数据
- 持久化层：存储数据
- 步骤：
  - 做controller 拦截来自ngnix的日志数据
  - 集群做负载均衡，并启动每台机器的controller
  - ngnix启动，mock的jar包产生模拟数据。

### 项目分层


#### ODS层(已跑通)

- ODS_BASE_LOG：通过springboot采集至kafka topic

  - 将日志数据原封不动采集至kafka topic

  - 数据格式：

    ```json
    {
    	"common":{
    		"ar":"...",//地区编码
    		"ba":"...",//客户端
    		"ch":"...",//渠道
    		"is_new":"...",//是否新用户
    		"md":"...",//机型
    		"mid":"...",
    		"os":"...",//系统
    		"uid":"...",//用户id
    		"vc":"..."//版本号
    	},
        "displays":{//曝光
        },
        "start":{//启动
        },
        "page":{//页面信息
        }
    }
    ```
    
    

- ODS_BASE_DB：通过flink cdc采集至kafka topic

  - 配置mysql binlog，监控gmall数据库

  - FlinkCDC配合binlog来获取到数据库每条数据的变化

  - flink cdc采集的多表数据到kafka都只会放到kafka的一个topic中
  
    ```java
    DebeziumSourceFunction<String> sourceFunction = MySQLSource.<String>builder()
                    .hostname("hadoop102")
                    .port(3306)
                    .username("root")
                    .password("000000")
                    .databaseList("gmall2022")
                    .deserializer(new CustomerDeserialization())
                    .startupOptions(StartupOptions.initial())
                    .build();
    ```
  
  - 数据格式序列化为json
  
    ```json
      {
      "database":"",
      "tableName":"",
      "before":{"id":"","tm_name":""....}, //更新前
      "after":{"id":"","tm_name":""....}, //更新后
      "type":"c u d", //增删改
      "ts":156456135615
      }
    ```
  

#### DWD/DIM层

- 行为数据
  - flink读取KAFKA ODS_BASE_LOG  这个topic
  - 过滤脏数据
  - 新老用户校验，获取状态数据，把状态中新用户is_new 的值改为 0，避免后续数据重复
  - 三个主题，写入到kafka topic
    - dwd_start_log  启动日志 topic
      - start != null 的数据
    - dwd_page_log 页面信息topic
      - 页面日志主流
    - dwd_display_log 曝光信息的topic
      - display != null 的数据
- 业务数据
  - flink 消费kafka obs_base_db这个 topic
  - Flink cdc只能把数据写入一个topic中，需要把每个表拆分处理，而且mysql会有新增表
  - 所以mysql做了动态配置方案binlog，Flinkcdc感知到配置表变化，自动采集广播流且因为mysql会有新增表，通过flinkcdc做了广播流处理
    - 步骤：
      - 在mysql创建数据库（不同于业务数据库），然后创建配置表
      - mysql开启binlog，添加配置表
      - 编写代码，读取mysql数据流，把mysql数据流转换为广播流
  - 主流和广播流数据连接然后分流
  - 维度表保存在HBase表中，作为DIM层
    - 地区维度，用户维度，SKU维度，SPU维度，TM维度，Categroy维度
  - 一张事实表对应kafka一个topic，作为DWD层，供dwm层使用
    - 订单表，订单明细表
    - 支付表

#### DWM层

##### 访客UV计算（日志流计算）

- 行为数据中有last_page_id，为null的数据保留，其他过滤

- 访客同一天多次登陆，1失效，需对1中按照天做过滤

  - 保留last_page_id = null 所有数据的状态（存储时间）

  - 当状态为null时，是该访客第一访问，保留数据，更新状态

  - 当状态不为null，比较状态与timestamp，如果不相等，保留数据，更新状态

- 状态设置ttl   24小时后自动删除，更新状态后重新计时UpdateType.OnCreateAndWrite

- 写入到kafka topic中dwm_unique_visit

##### 跳出明细（日志流计算）

- 跳出次数/访问次数
- 两个条件，Flink CEP实现
  - 上次访问 last_page_id 为null，就是跳出数据
  - 设置10s的会话窗口，在10s内没有数据的，也是跳出数据
- 写入到kafkatopic dwm_user_jump_detail

##### 订单宽表（关系数据库）

- Interval join：订单表join订单明细表，窗口大小（-5,5）
  - json转java bean
  - 设置水印
  - 两条流join
- 订单表join完成后关联维度信息
  - JDBCUtil：把phoenix维度表中每一行数据放进集合中，最后把集合封装成JSONObject
    - 问题：订单表bean字段驼峰命名，Phoenix表字段_命名，关联时需要装换名字
    - 解决：使用Guava包把"_"转为小驼峰
  - DimUtil：把Phoenix中的维度表加载到redis
    - 问题：Flink从Phoenix拿数据延迟太高，会产生反压
    - 解决：redis做为旁路缓存
      - DWD层DIM写入HBase时，如果进入的数据为更新操作，则删除redis的数据
  - DimAsyncFunction：异步IO访问phoenix
    - 问题：访问phoenix单线程处理缓慢
    - 解决：用线程池创建多线程去访问Phoenix异步IO
  - 关联的维度有：地区维度，用户维度，SKU维度，SPU维度，TM维度，Categroy维度
- 关联完成后输出的kafka

##### 支付宽表

- 双流Join，将订单宽表和支付表进行Join，得到支付宽表
  - interval join：支付表join订单宽表，窗口设置为-15，5
- 完成关联后将支付宽表输出到kafka



#### DWS层

##### 访客主题宽表

- 要求的指标

  - PV：dwd_page_log直接求

  - UV：dwm_unique_visit直接拿

  - 跳出率：dwm 跳出明细计算，跳出次数/访问次数

  - 进入页面数：dwd page_log计算

  - 连续访问时长：dwd_page_log中duration time做sum

- 涉及到的主题有dwd_page_log，dwm_unique_visit  dwm_user_jump_detail
- flink拿到这三个主题，并从主题中拿到以上指标信息，然后进行关联
- 关联后按照维度（渠道，地区，版本，新老用户）进行分组开窗聚合  10s的滚动窗口
- 数据写入clickhouse表visitor_stats

##### 商品主题宽表

- 指标
  - 点击，曝光，dwd_page_log
  - 收藏：收藏表
  - 加入购物车：购物车表
  - 下单：订单宽表  dwm_order_wide
  - 支付：支付宽表  dwm_payment_wide
  - 退款：退款表
  - 评价：评价表
- 把对应的指标数据拿到，然后把所有的数据合并为一个流，设置水位线
- 按照维度分组（SKU_ID,SPU_ID,品牌id,品类id），开窗10s，聚合
- 完成后添加到ck表product_stats

##### 地区主题宽表

- 指标
  - stt：窗口开始时间	
  - edt：窗口结束时间
  - 省份id，名称，代码
  - 订单总量，订单金额，时间戳
- 消费dmw_order_wide订单宽表
- 开窗10s，聚合
- 完成后添加到ck表



### 项目优化

##### 回答这个问题的套路

1. 说明业务场景
2. 遇到什么问题 --> 往往是通过监控工具和报警系统发现的问题
3. 排查问题
4. 解决手段
5. 问题被解决

##### 资源优化

在系统刚上线后的一次的活动中，因为QPS量过大，导致某些slot中的数据处理不过来，出现背压情况

重新评估了数据量，重新指定了资源配置规范

首先并行度：

在source，sink并行度=QPS/单并行度的处理能力*1.2

transform阶段根据算子进行并行度的设置

keyby之前的算子，一般都做一些map，filter操作，并行度与source相同即可

keyby之后的算子，如过聚合操作操作很多，设置并行度为2的整数次幂

然后内存：

jobmanager  2G

taskmanager 设置了 6G



##### **反压优化**

除了因配置问题出现反压以外，还会出现负载不均衡，GC导致性能问题出现反压

系统资源的反压出现后可以多给资源

负载不均衡会通过给key加盐或者实现本地聚合来减轻数据倾斜的影响

GC问题可以通过flink ui把gc日志下载，查看Full gc老年代剩余大小，重新推倒堆大小



##### **数据倾斜优化**

首先会出现kafka topic中分区数据分布不均匀，导致在flink处理中出现了数据倾斜，这种情况使用shuffle，rebalance等算子将数据均匀分配，解决数据倾斜问题

再keyby之后聚合发生的数据倾斜问题，可以用localkeyby的思想，两阶段聚合，现在本地将本地的数据聚合，然后再发送到下游进行聚合，这样可以减少下游进入的数据量





##### **Flink SQL调优**

- 开启miniBatch微批处理，缓存一定的数据后在触发处理，减少对state的访问，从而提升吞吐。
- 开启LocalGloabl，LocalGlobal可以将aggregate分成local+Global两阶段聚合，现在本地聚合后再全局聚合，这种方式可以减少直接全局聚合出现的热点问题。
- Count distinct场景下Case when可以用filter替换，可以减少状态的大小和对状态的访问。



##### 项目组件优化

在dwm层计算订单明细表，会用到phonix中的维度表，每次从phonix表中聚合时会有100ms+以上的延迟，当数据量太大时会无法及时进入窗口。

然后选择了redis做了旁路缓存，在维度表写入HBase时，会先写进redis一份，然后flink需要进行聚合时，首先从redis中拿维度数据，延迟会缩小到个位数。



### 项目资源

每日数据量：

平时 27万条左右，30G

每月活动促销   50万条，50G左右

表数量：300多张





### 附：Clickhouse

- clickhouse为什么快？
  - 列式存储，聚合查询很快
  - 有很多引擎，支持各种场景，最常用的MergeTree
    - 允许Ck是分布式的
    - 主键用的是稀疏索引
      - 占用空间小
      - 而且有序

- 列式存储优点

  - 聚合查询快

  - 是OLAP数据库

- OLAP和OLTP的区别

  - OLAP：分析决策使用的数据仓库，读操作比较多，且性能好

  - OLTP：处理事务的数据库，由原子事务组成

- 表引擎有哪些？

  - TinyLog：存储在磁盘上，不支持索引，没有并发，一般小表

  - Memory：内存引擎，不支持索引，性能高


- 表引擎MergeTree
  - MergeTree：最常用的一种ck表引擎
    - partition by：分区字段，MergeTree允许CK表是分布式的
    - primary key：必须是order by后面的字段：因为MergeTree性能高的原因是读取很快，有稀疏索引，而稀疏索引必须要对主键排序
      - 稀疏索引
        - 占用空间小，查询快
        - 必须是有序的
    - Order by ：指定排序字段
    - TTL：过期时间


- ReplacingMergeTree
  - 去重的MergeTree
  - 根据order by的字段进行去重
  - 使用方式：engine=ReplacingMergeTree(字段)
    - 字段：表示该条数据的版本，如果重复，保留这个字段版本最新的

- SummingMergeTree
  - 聚合的MergeTree
  - 使用方式：engine=SummingMergeTree(字段)
    - 字段：表示要聚合的字段

- CK SQL

  - with rollup：从右到 左去除维度进行小计

  - with cube：从右到 左去除维度进行小计，再从左到右去除维度进行小计

  - with totals：只计算合计



### 附：马老师简历

2020.12-2021.9 星光直播数据分析
大数据开发

项目背景：
随着疫情爆发以来，许多线上产业得到快速发展，其中直播电商就是其中之一，本项目为直播电商平台打造一个大数据智能平台，通过flink的实时计算能力，可为用户推荐最热门的主播咨询，为运营人员实时提供最新的销售业绩数据，及时调整直播方案。
技术选型：
Mysql、SpringBoot、Nginx、kafka、Flink、CK、Zeplin、DS、prometheus+Grafana、mysql、redis、hbase、kudu、impala
项目描述：
•数据主要来源分为日志数据和业务数据
•由日志服务器直接发送所有日志数据至kafka，作为ods层
•从kafka消费日志数据信息，进行分类，分为曝光日志、启动日志、页面日志，通过测数据对各类日志进行收集，实现日志分流操作，再写回至kafka，作为DWD层；
•通过FlinkCDC实时监控mysql数据库，增量采集中后端业务数据的93张数据表信息，写入至kafka作为ods层
•通过配置动态管理表来动态管理划分事实表和维度表，过滤后的事实表写入至kafka作为dwd层，而维度表则写入Hbase。
•进一步对各主题的业务数据进行拉宽和判断操作，再写入kafka，作为dwm中间层，为数据的复用性做准备。
•将kafka中的各层数据表同步至kudu离线数仓端
•对业务数据进行维度聚合，写入OLAP存储介质Clickhouse，通过sql查询和数据接口服务，实现大屏实时展示。
个人职责：
•开发环境的搭建
•负责工具类开发的统一管理
•实现了FlinkCDC实时业务数据监控
•实现了动态分配事实表和维度表的管理
•实现了维度表查询的优化，即对hbase访问优化
•实现了UV、PV、跳出率以及主播等主题的数仓层级构建
•实现了数据接口开发的工作
技术挑战：
因为直播行业的特殊性，导致该项目的数据吞吐量非常不平均，吞吐量在晚间7~9点达到高峰。其上限和下限的差距能达到数十倍。因此对于高峰期的访问量要求严苛，在高峰期查询效率明显降低
解决方案：
1 在查询Hbase的架构中，增设了热缓存机制，选用了redis作为缓存介质
2 使用Flink中的异步IO操作进行读取访问，以提高访问效率
技术挑战：
count(distinct)出现数据倾斜问题，以及空的数据，或者大量同样的数据，参与关联，或者参与partition by轻则影响性能，重则内存溢出报错。
解决方案：
分层次逐步减少数据量 
过滤掉倾斜的数据 
不能过滤想方设法让其均衡分布 
不能均衡分布就想办法让倾斜的数据做mapjoin 

。	
