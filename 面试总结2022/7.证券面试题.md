#### 遇到过什么问题？

1. 在设计HBase Rowkey的时候使用的是证券代码+时间的方式，自动分区时出现了热点问题，然后采用了手动分区的方式，按照证券代码的分布，重新进行了分区

#### 数据量

秒级行情：一天7000-8000w，存储五日内的数据量

日采集总量 40G

分时行情：一天数据量500万左右

离线数仓：一年数据量

- 秒级 210亿
- 分时 20亿
- K线   1500万



#### 机器配置

ResourceManager节点  6个

NodeManager 节点 27个 

#### 为什么选择Druid

- Druid做实时摄取和小数据量的预聚合速度非常快，证券项目分时窗口一分钟只拿最后一秒的数据做计算，股票债券基金这一秒数据量大概在2万条左右，选用Druid速度最快。

#### Druid如何使用

在配置文件（JSON）中指定数据源：数据源是kafka

指定指标计算规则，粒度规则

然后再webui上提交

#### 为什么选择AVRO

- 优点：数据结构丰富（基本类型8种，复杂类型），操作方便，几乎能满足所有场景
- 序列化传输
- 操作：json文本格式，封装成avsc文件，然后用官方提供的jar包编译生成avro对象

#### 整个开发流程：

##### 流处理：

Step1：沪深服务端持续发送IO流给客户端，在开发阶段，开发服务端模拟传输数据

Step2：开发客户端，客户端做两件事，1.接收数据，2.作为Kafka生产者

- 接收数据
  - 建立socket，获取流数据，将数据转为AVRO对象
    - 按照行数据格式封装avsc文件，然后编译生成avro对象

- 作为Kafka生产者，把数据生产到指定的topic中

Step3：Flink消费kafka数据

- 消费sse topic中的数据，并AVRO反序列化

Step4：业务阶段（个股行情，板块行情）

- 数据ETL

  - 数据过滤

    - 过滤掉事件时间再9.30-3.30之外的数据

    - 过滤掉高开低收为0的数据

  - 数据转换

    - 数据过滤完成后封装成Bean对象

- 个股业务，过滤数据，个股标识是MD002

- 设置水位线2s

- 处理子业务，子业务要公用一个数据集，设置一个接口解耦，子业务去重写方法

  - 秒级行情（个股行情，板块行情）
    - 设置5s窗口
    - 然后用apply处理出秒级行情的数据
    - 封装成bean对象写入HBase
    - 在HBase端建表，设计rowkey，手动分区
  - 分时行情（个股行情，板块行情）
    - 用侧边流分流数据，分表存储
    - 取每分钟最后一秒59s的数据进行计算
    - 封装成bean对象并AVRO写入kafka对应的topic
    - druid开启摄取任务，消费kafka数据
  - K线行情（个股行情）
  - 涨跌幅行情
  - 分时数据备份
    - 将这些数据同时写入HDFS

Step3：配置流处理环境（开发时不开启）

- 设置并行度：和kafka topic分区数一样

- 设置检查点，设置检查后端
- 设置重启机制

- 

  



##### 批处理

批处理输出的有个股K线表

指数K线表

板块成分股对应关系表等等

用这些表与实时流处理的数据进行聚合计算，得到周K，月K等信息。

